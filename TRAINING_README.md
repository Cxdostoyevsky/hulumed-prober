# ğŸ¥ Hulu-Med BUSI ä¹³è…ºç™Œå¾®è°ƒæŒ‡å—

## ğŸ“‹ é¡¹ç›®è¯´æ˜
æœ¬æŒ‡å—å¸®åŠ©ä½ ä½¿ç”¨ **LoRA/QLoRA** åœ¨ **BUSI ä¹³è…ºç™Œæ•°æ®é›†**ä¸Šå¾®è°ƒ Hulu-Med-4B æ¨¡å‹ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡
ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼š
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ æ•°æ®å‡†å¤‡
ç¡®è®¤ä»¥ä¸‹æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼š
- **è®­ç»ƒæ•°æ®**: `/Users/chenxi/Desktop/Hulu-Med/train_busi_breast_cancer.json`
- **å›¾åƒæ ¹ç›®å½•**: `/ssd/chenxi/Hulu-Med/BUSI/BUSI`
- **é¢„è®­ç»ƒæ¨¡å‹**: `/ssd/common/LLMs/Hulu-Med-4B`

å¦‚æœè·¯å¾„ä¸åŒï¼Œè¯·ä¿®æ”¹ `train_busi_lora.py` ä¸­çš„ä»¥ä¸‹ä½ç½®ï¼š
```python
model_path: str = field(default="/ssd/common/LLMs/Hulu-Med-4B")
train_json: str = field(default="/Users/chenxi/Desktop/Hulu-Med/train_busi_breast_cancer.json")
image_root: str = field(default="/ssd/chenxi/Hulu-Med/BUSI/BUSI")
```

### 3ï¸âƒ£ å¼€å§‹è®­ç»ƒ

#### æ–¹å¼ Aï¼šå•å¡è®­ç»ƒ
```bash
CUDA_VISIBLE_DEVICES=1 python train_busi_lora.py
```

#### æ–¹å¼ Bï¼šå¤šå¡è®­ç»ƒï¼ˆæ¨èï¼‰
```bash
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 train_busi_lora.py
```

#### æ–¹å¼ Cï¼šä½¿ç”¨ DeepSpeed ZeRO-2ï¼ˆå¤šå¡ + èŠ‚çœæ˜¾å­˜ï¼‰
```bash
# 1. ä¿®æ”¹ train_busi_lora.py ä¸­çš„ TrainingArgumentsï¼Œå–æ¶ˆæ³¨é‡Šï¼š
# deepspeed="./ds_config_zero2.json",

# 2. è¿è¡Œè®­ç»ƒ
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 train_busi_lora.py
```

---

## âš™ï¸ è¶…å‚æ•°è¯´æ˜

### æ ¸å¿ƒè®­ç»ƒå‚æ•°ï¼ˆåœ¨ `train_busi_lora.py` ä¸­ä¿®æ”¹ï¼‰

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `num_train_epochs` | `3` | è®­ç»ƒè½®æ•° |
| `per_device_train_batch_size` | `2` | æ¯å¼ å¡çš„ batch size |
| `gradient_accumulation_steps` | `8` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆç›¸å½“äº batch=16ï¼‰ |
| `learning_rate` | `2e-4` | å­¦ä¹ ç‡ï¼ˆLoRA æ¨è 1e-4~5e-4ï¼‰ |
| `warmup_ratio` | `0.03` | Warmup æ¯”ä¾‹ï¼ˆå‰ 3% çš„ stepï¼‰ |
| `weight_decay` | `0.01` | æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰ |
| `lr_scheduler_type` | `"cosine"` | å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰ |
| `max_grad_norm` | `1.0` | æ¢¯åº¦è£å‰ªé˜ˆå€¼ |

### LoRA å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `use_lora` | `True` | æ˜¯å¦ä½¿ç”¨ LoRA |
| `use_qlora` | `False` | æ˜¯å¦ä½¿ç”¨ QLoRAï¼ˆ4bit é‡åŒ–ï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰ |
| `lora_r` | `64` | LoRA ç§©ï¼ˆè¶Šå¤§å‚æ•°è¶Šå¤šï¼Œå»ºè®® 32~128ï¼‰ |
| `lora_alpha` | `128` | LoRA ç¼©æ”¾å› å­ï¼ˆä¸€èˆ¬è®¾ä¸º 2*rï¼‰ |
| `lora_dropout` | `0.05` | LoRA Dropout æ¯”ä¾‹ |
| `lora_target_modules` | `q_proj,v_proj,...` | è¦åº”ç”¨ LoRA çš„æ¨¡å— |
| `lora_exclude_patterns` | `vision_encoder` | æ’é™¤çš„æ¨¡å—ï¼ˆé¿å…ç»™è§†è§‰ç¼–ç å™¨åŠ  LoRAï¼‰ |
| `train_mm_projector` | `False` | æ˜¯å¦åŒæ—¶è®­ç»ƒæ˜ å°„å±‚ mm_projectorï¼ˆè§†è§‰â†’LLM æŠ•å½±å±‚ï¼‰ |

---

## ğŸ’¾ æ˜¾å­˜ä¼˜åŒ–å»ºè®®

### æ˜¾å­˜ä¸è¶³ï¼Ÿè¯•è¯•è¿™äº›æ–¹æ³•ï¼š

#### 1. **å¯ç”¨ QLoRAï¼ˆ4bit é‡åŒ–ï¼‰**
ä¿®æ”¹ `ModelArguments` ä¸­çš„å‚æ•°ï¼š
```python
use_qlora: bool = field(default=True)  # æ”¹ä¸º True
```

#### 2. **å‡å° batch size**
```python
per_device_train_batch_size=1,  # æ”¹ä¸º 1
gradient_accumulation_steps=16,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
```

#### 3. **å‡å° LoRA ç§©**
```python
lora_r: int = field(default=32)  # ä» 64 æ”¹ä¸º 32
```

#### 4. **å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼ˆåœ¨ `TrainingArguments` ä¸­æ·»åŠ ï¼‰
```python
gradient_checkpointing=True,
```

#### 5. **ä½¿ç”¨ 8bit ä¼˜åŒ–å™¨**
```python
optim="paged_adamw_8bit",
```

---

## ğŸ“Š è®­ç»ƒç›‘æ§

### TensorBoard å®æ—¶æŸ¥çœ‹
```bash
tensorboard --logdir=/ssd/chenxi/Hulu-Med/checkpoints/busi_lora/runs
```

### æˆ–ä½¿ç”¨ WandBï¼ˆéœ€åœ¨ `TrainingArguments` ä¸­è®¾ç½®ï¼‰
```python
report_to="wandb",
```
ç„¶åè¿è¡Œï¼š
```bash
wandb login  # é¦–æ¬¡ä½¿ç”¨éœ€è¦ç™»å½•
python train_busi_lora.py
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹ä¼šä¿å­˜åœ¨ï¼š
```
/ssd/chenxi/Hulu-Med/checkpoints/busi_lora/
â”œâ”€â”€ checkpoint-100/       # ä¸­é—´æ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint-200/
â”œâ”€â”€ final_model/          # æœ€ç»ˆæ¨¡å‹
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ tokenizeré…ç½®æ–‡ä»¶
â””â”€â”€ runs/                 # TensorBoard æ—¥å¿—
```

---

## ğŸ§ª è®­ç»ƒåæ¨ç†

ä¿®æ”¹ `inference_hulumed_qwen3.py` åŠ è½½ä½ çš„ LoRA æ¨¡å‹ï¼š

```python
# åœ¨ load_pretrained_model ä¹‹å‰æ·»åŠ ï¼š
from peft import PeftModel

# åŠ è½½åŸºç¡€æ¨¡å‹
tokenizer, model, image_processor, context_len = load_pretrained_model(...)

# åŠ è½½ LoRA æƒé‡
model = PeftModel.from_pretrained(
    model, 
    "/ssd/chenxi/Hulu-Med/checkpoints/busi_lora/final_model"
)
model = model.merge_and_unload()  # åˆå¹¶æƒé‡
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: `CUDA out of memory` é”™è¯¯
**A**: å‚è€ƒä¸Šé¢çš„"æ˜¾å­˜ä¼˜åŒ–å»ºè®®"ï¼Œä¾æ¬¡å°è¯•ï¼šQLoRA â†’ å‡å° batch size â†’ å‡å° LoRA ç§©ã€‚

### Q2: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢ï¼Ÿ
**A**: 
1. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† `bf16=True`ï¼ˆæˆ– `fp16=True`ï¼‰
2. å¦‚æœæœ‰å¤šå¼ å¡ï¼Œä½¿ç”¨ `torchrun` å¤šå¡è®­ç»ƒ
3. å¯ç”¨ DeepSpeed ZeRO-2

### Q3: å¦‚ä½•è°ƒæ•´è®­ç»ƒè½®æ•°ï¼Ÿ
**A**: ä¿®æ”¹ `num_train_epochs`ï¼Œæˆ–ä½¿ç”¨ `max_steps` æ›¿ä»£ï¼š
```python
max_steps=1000,  # è®­ç»ƒ 1000 æ­¥ååœæ­¢
num_train_epochs=None,  # è®¾ä¸º None
```

### Q4: æƒ³è¦ä¿å­˜æ›´å¤š checkpointï¼Ÿ
**A**: ä¿®æ”¹ `save_total_limit`ï¼š
```python
save_total_limit=5,  # ä¿ç•™æœ€æ–°çš„ 5 ä¸ª checkpoint
```

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ
- æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼š`cat /ssd/chenxi/Hulu-Med/checkpoints/busi_lora/trainer_state.json`
- æ£€æŸ¥æ¨¡å‹é…ç½®ï¼š`cat /ssd/common/LLMs/Hulu-Med-4B/config.json`

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ‰
