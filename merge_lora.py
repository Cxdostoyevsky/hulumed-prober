import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os
import shutil

# ================= é…ç½®åŒºåŸŸ =================
# 1. åŸå§‹åº•åº§æ¨¡å‹è·¯å¾„
BASE_MODEL_PATH = "/ssd/common/LLMs/Hulu-Med-4B"

# 2. è®­ç»ƒå¥½çš„ LoRA è·¯å¾„ (åŒ…å« adapter_model.safetensors çš„æ–‡ä»¶å¤¹)
ADAPTER_PATH = "/ssd/common/LLMs/Hulu-Med-4B_finetune/normal/final_model"

# 3. åˆå¹¶åçš„æ¨¡å‹ä¿å­˜è·¯å¾„ (ä½ å¯ä»¥ä¿®æ”¹è¿™ä¸ª)
OUTPUT_PATH = "/ssd/common/LLMs/Hulu-Med-4B_finetune/normal/Hulumed-4B-merged"


# ===========================================

def merge_model():
    print(f"ğŸš€ å¼€å§‹åˆå¹¶æ¨¡å‹...")
    print(f"ğŸ“‚ Base Model: {BASE_MODEL_PATH}")
    print(f"ğŸ“‚ Adapter:    {ADAPTER_PATH}")

    # 1. åŠ è½½ Base Model
    # æ³¨æ„ï¼šåˆå¹¶æ—¶å¿…é¡»ä»¥éé‡åŒ–å½¢å¼åŠ è½½ (float16 æˆ– bfloat16)ï¼Œä¸èƒ½ç”¨ load_in_4bit/8bit
    print("\nâ³ æ­£åœ¨åŠ è½½ Base Model (è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            torch_dtype=torch.float16,  # å»ºè®®ä½¿ç”¨ float16 èŠ‚çœæ˜¾å­˜
            device_map="auto",  # è‡ªåŠ¨åˆ†é…æ˜¾å¡æˆ– CPU
            trust_remote_code=True  # å…è®¸åŠ è½½è‡ªå®šä¹‰æ¨¡å‹ä»£ç 
        )
    except Exception as e:
        print(f"âŒ åŠ è½½ Base Model å¤±è´¥: {e}")
        return

    # 2. åŠ è½½ Tokenizer
    print("â³ æ­£åœ¨åŠ è½½ Tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            BASE_MODEL_PATH,
            trust_remote_code=True
        )
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ Tokenizer è­¦å‘Š (å¯èƒ½éœ€è¦æ‰‹åŠ¨å¤åˆ¶): {e}")
        tokenizer = None

    # 3. åŠ è½½ LoRA Adapter
    # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨å¤„ç† lora æƒé‡åˆå¹¶ä»¥åŠ modules_to_save (mm_projector) çš„æ›¿æ¢
    print("â³ æ­£åœ¨åŠ è½½ LoRA Adapter...")
    try:
        model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    except Exception as e:
        print(f"âŒ åŠ è½½ Adapter å¤±è´¥: {e}")
        return

    # 4. æ‰§è¡Œåˆå¹¶
    print("\nğŸ”„ æ­£åœ¨æ‰§è¡Œ merge_and_unload (åˆå¹¶æƒé‡)...")
    model = model.merge_and_unload()

    # 5. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {OUTPUT_PATH}")
    if not os.path.exists(OUTPUT_PATH):
        os.makedirs(OUTPUT_PATH)

    # å…ˆä¿å­˜æ–°ç”Ÿæˆçš„æƒé‡å’Œé…ç½®
    model.save_pretrained(OUTPUT_PATH)

    if tokenizer:
        tokenizer.save_pretrained(OUTPUT_PATH)
        print("âœ… Tokenizer å·²ä¿å­˜")
    else:
        print("âš ï¸ è¯·è®°å¾—æ‰‹åŠ¨å¤åˆ¶ tokenizer ç›¸å…³æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•")

    # 6. ä» Base Model å¤åˆ¶å…¶ä»–æ–‡ä»¶å¹¶è¦†ç›–é…ç½®
    print("\nğŸ“¦ æ­£åœ¨ä» Base Model å¤åˆ¶æ–‡ä»¶å¹¶è¦†ç›–éæƒé‡æ–‡ä»¶...")
    
    # éå† Base Model ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for item in os.listdir(BASE_MODEL_PATH):
        src_path = os.path.join(BASE_MODEL_PATH, item)
        dst_path = os.path.join(OUTPUT_PATH, item)
        
        # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœæ˜¯æƒé‡æ–‡ä»¶ï¼ˆ.safetensors æˆ– .index.jsonï¼‰ï¼Œè·³è¿‡å¤åˆ¶ï¼Œä¿ç•™æ–°ç”Ÿæˆçš„
        if item.endswith(".safetensors") or item == "model.safetensors.index.json":
            continue
            
        # å…¶ä»–æ‰€æœ‰æ–‡ä»¶ï¼ˆåŒ…æ‹¬ config.json, tokenizer.model ç­‰ï¼‰ï¼Œå¼ºåˆ¶ä» Base å¤åˆ¶å¹¶è¦†ç›–
        if os.path.isfile(src_path):
            shutil.copy2(src_path, dst_path)
            print(f"  - å·²è¦†ç›–/å¤åˆ¶æ–‡ä»¶: {item}")
        elif os.path.isdir(src_path):
            # å¦‚æœæœ‰å­æ–‡ä»¶å¤¹ï¼ˆå¦‚ vision_encoderï¼‰ï¼Œé€’å½’å¤åˆ¶
            if os.path.exists(dst_path):
                shutil.rmtree(dst_path)
            shutil.copytree(src_path, dst_path)
            print(f"  - å·²è¦†ç›–/å¤åˆ¶ç›®å½•: {item}")

    print(f"\nğŸ‰ åˆå¹¶åŠæ–‡ä»¶æ•´ç†å®Œæˆï¼ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ {OUTPUT_PATH} è¿›è¡Œæ¨ç†äº†ã€‚")


if __name__ == "__main__":
    merge_model()
