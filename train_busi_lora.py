"""
Hulu-Med ä¹³è…ºç™Œåˆ†ç±»å¾®è°ƒè®­ç»ƒè„šæœ¬ (LoRA/QLoRA)
æ”¯æŒå¤šå¡è®­ç»ƒã€æ¢¯åº¦ç´¯ç§¯ã€DeepSpeed åŠ é€Ÿ
"""
import sys
import os
# [é‡è¦] åœ¨å¯¼å…¥ bitsandbytes ä¹‹å‰è®¾ç½®ï¼šè‹¥ PyTorch ä¸º CUDA 12.8 è€Œ bitsandbytes æ— é¢„ç¼–è¯‘ 128ï¼Œ
# å¯å¼ºåˆ¶ä½¿ç”¨ 12.4 çš„äºŒè¿›åˆ¶ï¼ˆéœ€åœ¨ import ä»»ä½•ä¼šè§¦å‘ peft/bitsandbytes çš„åŒ…ä¹‹å‰ï¼‰
if "BNB_CUDA_VERSION" not in os.environ:
    os.environ["BNB_CUDA_VERSION"] = "124"
sys.path.append(os.path.join(os.getcwd(), "src"))
import json
import torch
from torch.utils.data import Dataset
from dataclasses import dataclass, field
from typing import Optional, Dict, List
from PIL import Image

from transformers import (
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    AutoTokenizer,
    AutoConfig,
)


try:
    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
except Exception as e:
    raise RuntimeError(
        "å¯¼å…¥ peft å¤±è´¥ï¼Œé€šå¸¸æ˜¯ bitsandbytes/triton ä¸å½“å‰ CUDA/PyTorch ä¸å…¼å®¹å¯¼è‡´ã€‚\n"
        "å¦‚æœä½ å½“å‰ä¸ç”¨ QLoRAï¼ˆuse_qlora=Falseï¼‰ï¼Œå¯ç›´æ¥æ‰§è¡Œï¼š\n"
        "  pip uninstall -y bitsandbytes triton\n"
        "è‹¥è¦ä½¿ç”¨ QLoRAï¼Œè¯·å®‰è£…ä¸å½“å‰ PyTorch CUDA ç‰ˆæœ¬åŒ¹é…çš„ bitsandbytes/tritonã€‚"
    ) from e


from hulumed_qwen3.model import load_pretrained_model
from hulumed_qwen3.model.processor import HulumedProcessor
from hulumed_qwen3.mm_utils import load_images, get_model_name_from_path


# ==================== 1. æ•°æ®é›†ç±» ====================
class BUSIDataset(Dataset):
    """BUSI ä¹³è…ºç™Œæ•°æ®é›†åŠ è½½å™¨"""

    def __init__(
            self,
            json_path: str,
            image_root: str,
            processor: HulumedProcessor,
            tokenizer,
            max_length: int = 2048,
    ):
        with open(json_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        self.image_root = image_root
        self.processor = processor
        self.tokenizer = tokenizer
        self.max_length = max_length

        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.data)} æ¡è®­ç»ƒæ•°æ®")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # 1. åŠ è½½å›¾åƒ
        image_path = os.path.join(self.image_root, item["image"])
        try:
            image_data = load_images(image_path)
        except Exception as e:
            print(f"âš ï¸  å›¾åƒåŠ è½½å¤±è´¥: {image_path}, é”™è¯¯: {e}")
            # è¿”å›ä¸€ä¸ªç©ºç™½å›¾åƒå ä½
            image_data = Image.new('RGB', (224, 224), (0, 0, 0))

        # 2. æ„å»ºå¯¹è¯æ ¼å¼ï¼ˆæ¨¡ä»¿æ¨ç†æ—¶çš„æ ¼å¼ï¼‰
        question = item["text"]
        answer = item["answer"]

        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": question},
                ]
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": answer}
                ]
            }
        ]

        # 3. ä½¿ç”¨ processor å¤„ç†
        inputs = self.processor(
            images=[image_data],
            text=conversation,
            merge_size=1,  # 2D å›¾åƒ
            return_tensors="pt",
            padding="max_length",
            max_length=self.max_length,
            truncation=True,
            return_labels=True,  # è®­ç»ƒæ—¶éœ€è¦ labels
        )

        # 4. æ¢é’ˆäºŒåˆ†ç±»æ ‡ç­¾ï¼šNormal=0, Benign/Malignant=1,2
        probe_label = item.get("label", 0)
        # if probe_label not in (0, 1):
        #     probe_label = 1 if probe_label >= 1 else 0
        inputs["probe_labels"] = torch.tensor(probe_label, dtype=torch.long)

        # 4. å°† batch ç»´åº¦å»æ‰ï¼ˆDataset è¿”å›å•æ¡æ•°æ®ï¼‰
        inputs = {k: v.squeeze(0) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}

        return inputs


# ==================== 2. æ•°æ®æ•´ç†å™¨ï¼ˆData Collatorï¼‰====================
@dataclass
class DataCollatorForHulumed:
    """è‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ï¼Œå¤„ç†å˜é•¿åºåˆ—å’Œå›¾åƒ"""
    tokenizer: AutoTokenizer

    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:
        batch = {}
        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else 0

        # æ–‡æœ¬å­—æ®µï¼šåŠ¨æ€è¡¥é½åˆ°å½“å‰ batch çš„æœ€å¤§é•¿åº¦ï¼Œé¿å…ä¸åŒæ ·æœ¬é•¿åº¦å¯¼è‡´ stack å¤±è´¥
        if "input_ids" in features[0]:
            input_ids = [f["input_ids"].view(-1) for f in features]
            batch["input_ids"] = torch.nn.utils.rnn.pad_sequence(
                input_ids, batch_first=True, padding_value=pad_token_id
            )

        if "attention_mask" in features[0]:
            attention_masks = [f["attention_mask"].view(-1) for f in features]
            batch["attention_mask"] = torch.nn.utils.rnn.pad_sequence(
                attention_masks, batch_first=True, padding_value=0
            )
        elif "input_ids" in batch:
            batch["attention_mask"] = (batch["input_ids"] != pad_token_id).long()

        if "labels" in features[0]:
            labels = [f["labels"].view(-1) for f in features]
            batch["labels"] = torch.nn.utils.rnn.pad_sequence(
                labels, batch_first=True, padding_value=-100
            )

        # å›¾åƒå­—æ®µï¼šHuluMed ä½¿ç”¨åŠ¨æ€ resizeï¼Œä¸åŒå›¾åƒçš„ patch æ•°é‡ä¸åŒï¼ˆå¦‚ [1599,588] vs [1400,588]ï¼‰
        # ä¸èƒ½ stackï¼Œéœ€æ²¿ dim=0 æ‹¼æ¥ï¼›æ¨¡å‹é€šè¿‡ grid_sizes æŒ‰æ ·æœ¬åˆ‡åˆ†
        if "pixel_values" in features[0]:
            batch["pixel_values"] = torch.cat([f["pixel_values"] for f in features], dim=0)
        if "grid_sizes" in features[0]:
            gs_list = [f["grid_sizes"].squeeze().view(-1) for f in features]
            batch["grid_sizes"] = torch.stack(gs_list)  # [B, 3]
        if "merge_sizes" in features[0]:
            ms_list = [f["merge_sizes"].squeeze() for f in features]
            batch["merge_sizes"] = torch.stack(ms_list)  # [B]
        if "probe_labels" in features[0]:
            batch["probe_labels"] = torch.stack([f["probe_labels"] for f in features])

        return batch

        return batch


# ==================== 3. è®­ç»ƒé…ç½® ====================
def get_lora_target_modules(model, target_suffixes: List[str], exclude_patterns: List[str] = None):
    """
    è·å– LoRA ç›®æ ‡æ¨¡å—åˆ—è¡¨ï¼Œæ”¯æŒæ’é™¤æŒ‡å®šæ¨¡å—ï¼ˆå¦‚è§†è§‰ç¼–ç å™¨ï¼‰ã€‚

    å› ä¸º vision_encoder é‡Œä¹Ÿæœ‰ q_proj/v_proj/k_proj ç­‰ï¼Œè‹¥ç›´æ¥ç”¨ target_suffixesï¼Œ
    PEFT ä¼šåŒæ—¶ç»™è§†è§‰ç¼–ç å™¨å’Œ LLM åŠ  LoRAã€‚æœ¬å‡½æ•°é€šè¿‡ exclude_patterns æ’é™¤ vision_encoderã€‚
    """
    exclude_patterns = exclude_patterns or ["vision_encoder"]

    module_names = []
    for name, module in model.named_modules():
        if not isinstance(module, torch.nn.Linear):
            continue
        if any(name.endswith(suffix) for suffix in target_suffixes):
            if not any(exc in name for exc in exclude_patterns):
                module_names.append(name)

    return list(set(module_names))  # å»é‡


@dataclass
class ModelArguments:
    model_path: str = field(default="/ssd/common/LLMs/Hulu-Med-4B_finetune/normal/Hulu-Med-4B-merge-50-epoc")
    use_lora: bool = field(default=True)
    use_qlora: bool = field(default=False)  # å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œæ”¹ä¸º True
    lora_r: int = field(default=64)
    lora_alpha: int = field(default=128)
    lora_dropout: float = field(default=0.05)
    lora_target_modules: str = field(default="q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj")
    lora_exclude_patterns: str = field(default="vision_encoder")  # æ’é™¤çš„æ¨¡å—è·¯å¾„ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œé¿å…ç»™è§†è§‰ç¼–ç å™¨åŠ  LoRA
    train_mm_projector: bool = field(default=True)  # æ˜¯å¦åŒæ—¶è®­ç»ƒæ˜ å°„å±‚ mm_projectorï¼ˆè§†è§‰ç‰¹å¾â†’LLM çš„æŠ•å½±å±‚ï¼‰
    # æ¢é’ˆé…ç½®ï¼švision_encoder_0, vision_encoder_5, projector, llm_0, llm_12 ç­‰ï¼›None è¡¨ç¤ºä¸ä½¿ç”¨æ¢é’ˆ
    # vision_encoder:0-26, llm:0-35, projector
    probe_position: Optional[str] = field(default=None)
    probe_type: str = field(default="mlp")
    probe_hidden_dim: Optional[int] = field(default=None)
    # è®­ç»ƒè¾“å‡ºç›®å½•ä¸è¿è¡Œåï¼ˆä¾› run_train.sh ä¼ å…¥ï¼Œé¿å…å¤šä»»åŠ¡å†™åŒä¸€è·¯å¾„ï¼‰
    output_dir: str = field(default="/ssd/common/LLMs/Hulu-Med-4B_finetune/prober_4")
    run_name: str = field(default="hulu_med_busi_lora")


@dataclass
class DataArguments:
    train_json: str = field(default="/ssd/chenxi/Hulu-Med/BUSI/BUSI/train_busi_breast_cancer.json")
    image_root: str = field(default="/ssd/chenxi/Hulu-Med/BUSI/BUSI")
    max_length: int = field(default=2048)


# ==================== 4. ä¸»è®­ç»ƒå‡½æ•° ====================
def train(model_args: ModelArguments, data_args: DataArguments):
    # --- 4.1 å‚æ•°é…ç½®ï¼ˆmodel_args/data_args ç”±å‘½ä»¤è¡Œè§£æï¼Œtraining_args å›ºå®šï¼‰---
    training_args = TrainingArguments(
        # åŸºç¡€é…ç½®
        output_dir=model_args.output_dir,
        run_name=model_args.run_name,

        # è®­ç»ƒè¶…å‚æ•°
        num_train_epochs=50,  # è®­ç»ƒè½®æ•°
        per_device_train_batch_size=1,  # æ¯å¼ å¡çš„ batch sizeï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰
        gradient_accumulation_steps=8,  # æ¢¯åº¦ç´¯ç§¯ï¼ˆç›¸å½“äº batch_size=2*8=16ï¼‰
        learning_rate=2e-4,  # å­¦ä¹ ç‡ï¼ˆLoRA å»ºè®® 1e-4 åˆ° 5e-4ï¼‰
        lr_scheduler_type="cosine",  # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
        warmup_ratio=0.03,  # Warmup æ¯”ä¾‹ï¼ˆå‰ 3% çš„ step åš warmupï¼‰
        weight_decay=0.01,  # æƒé‡è¡°å‡
        max_grad_norm=1.0,  # æ¢¯åº¦è£å‰ª

        # ä¼˜åŒ–å™¨é…ç½®
        optim="adamw_torch",  # ä¼˜åŒ–å™¨ï¼ˆæˆ–ç”¨ "paged_adamw_8bit" èŠ‚çœæ˜¾å­˜ï¼‰
        adam_beta1=0.9,
        adam_beta2=0.999,
        adam_epsilon=1e-8,

        # æ—¥å¿—å’Œä¿å­˜
        logging_steps=10,
        save_steps=100,
        save_total_limit=1,  # åªä¿ç•™æœ€æ–°çš„ 1 ä¸ª checkpoint
        eval_strategy="no",  # ä¸åšéªŒè¯ï¼ˆå¦‚æœæœ‰éªŒè¯é›†æ”¹ä¸º "steps"ï¼‰

        # æ•°æ®ç±»å‹å’Œè®¾å¤‡
        bf16=True,  # ä½¿ç”¨ BF16ï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰
        fp16=False,  # å¦‚æœä¸æ”¯æŒ BF16ï¼Œæ”¹ä¸º fp16=True
        dataloader_num_workers=4,
        dataloader_pin_memory=True,

        # DeepSpeed é…ç½®ï¼ˆå¦‚æœè¦ç”¨ ZeRO-2/3ï¼Œå–æ¶ˆæ³¨é‡Šï¼‰
        # deepspeed="./ds_config_zero2.json",

        # å…¶ä»–
        remove_unused_columns=False,  # ä¿ç•™å›¾åƒæ•°æ®
        ddp_find_unused_parameters=False,  # å¤šå¡è®­ç»ƒä¼˜åŒ–
        report_to="tensorboard",  # æ—¥å¿—è®°å½•åˆ° TensorBoardï¼ˆæˆ–æ”¹ä¸º "wandb"ï¼‰
    )
    os.makedirs(training_args.output_dir, exist_ok=True)
    print(f"ğŸ“ å½“å‰è®­ç»ƒè¾“å‡ºç›®å½•: {training_args.output_dir}")

    # --- 4.2 åŠ è½½æ¨¡å‹ ---
    print("ğŸš€ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    model_name = get_model_name_from_path(model_args.model_path)

    # å¦‚æœä½¿ç”¨ QLoRAï¼Œéœ€è¦ 4bit é‡åŒ–
    load_4bit = model_args.use_qlora

    tokenizer, model, image_processor, context_len = load_pretrained_model(
        model_args.model_path,
        None,
        model_name,
        load_4bit=load_4bit,
        device_map='auto',  # è‡ªåŠ¨åˆ†é…åˆ°å¤šå¡
        trust_remote_code=True,
        attn_implementation="sdpa",  # æˆ–æ”¹ä¸º "flash_attention_2"
    )

    processor = HulumedProcessor(image_processor, tokenizer)
    model.config.use_cache = False  # è®­ç»ƒæ—¶å¿…é¡»å…³é—­ KV Cache
    model.config.use_token_compression = True

    # --- 4.2.1 é…ç½®æ¢é’ˆï¼ˆè‹¥å¯ç”¨ï¼‰---
    if model_args.probe_position is not None:
        model.config.probe_position = model_args.probe_position
        model.config.probe_type = model_args.probe_type
        model.config.probe_hidden_dim = model_args.probe_hidden_dim
        meta_model = model.model if hasattr(model, 'model') else model
        if hasattr(meta_model, 'model'):
            meta_model = meta_model.model
        if getattr(meta_model, 'probe', None) is None:
            from hulumed_qwen3.model.probe import ProbeHead
            probe_input_dim = meta_model._get_probe_input_dim(meta_model.config, model_args.probe_position)
            meta_model.probe = ProbeHead(
                input_dim=probe_input_dim,
                num_classes=3,
                probe_type=model_args.probe_type,
                hidden_dim=model_args.probe_hidden_dim,
            )
            meta_model.probe.to(model.device)
        print(f"ğŸ”¬ æ¢é’ˆå·²å¯ç”¨: position={model_args.probe_position}, type={model_args.probe_type}")

    # --- 4.3 é…ç½®è®­ç»ƒæ¨¡å¼ (æ¢é’ˆè®­ç»ƒ vs LoRA å¾®è°ƒ) ---
    if model_args.probe_position is not None:
        print("ğŸ”¬ æ­£åœ¨é…ç½®æ¢é’ˆè®­ç»ƒæ¨¡å¼ï¼ˆå†»ç»“å…¶ä½™æ¨¡å—ï¼‰...")
        # 1. å†»ç»“æ‰€æœ‰å‚æ•°
        for param in model.parameters():
            param.requires_grad = False

        # 2. åªå¼€å¯æ¢é’ˆå‚æ•°çš„æ¢¯åº¦
        meta_model = model.model if hasattr(model, 'model') else model
        if hasattr(meta_model, 'model'):
            meta_model = meta_model.model
        if hasattr(meta_model, 'probe'):
            for param in meta_model.probe.parameters():
                param.requires_grad = True

        # æ‰“å°å¯è®­ç»ƒå‚æ•°é‡
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        all_params = sum(p.numel() for p in model.parameters())
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params} / {all_params} ({100 * trainable_params / all_params:.4f}%)")

    elif model_args.use_lora or model_args.use_qlora:
        print("ğŸ”§ æ­£åœ¨é…ç½® LoRA å¾®è°ƒ...")

        if model_args.use_qlora:
            model = prepare_model_for_kbit_training(model)

        # æ¡ä»¶è¿‡æ»¤ï¼šåªå¯¹ LLM åŠ  LoRAï¼Œæ’é™¤ vision_encoderï¼ˆè§†è§‰ç¼–ç å™¨é‡Œä¹Ÿæœ‰ q_proj/v_proj ç­‰ï¼‰
        target_suffixes = [s.strip() for s in model_args.lora_target_modules.split(",")]
        exclude_patterns = [s.strip() for s in model_args.lora_exclude_patterns.split(",") if s.strip()]
        target_module_list = get_lora_target_modules(model, target_suffixes, exclude_patterns)
        print(f"   LoRA ç›®æ ‡æ¨¡å—æ•°: {len(target_module_list)} (å·²æ’é™¤: {exclude_patterns})")
        modules_to_save = ["model.mm_projector"] if model_args.train_mm_projector else None

        lora_config = LoraConfig(
            r=model_args.lora_r,
            lora_alpha=model_args.lora_alpha,
            target_modules=target_module_list,
            lora_dropout=model_args.lora_dropout,
            bias="none",
            task_type="CAUSAL_LM",
            modules_to_save=modules_to_save,
        )

        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°é‡

    for name, param in model.named_parameters():
        print(f"{name}: requires_grad={param.requires_grad}, shape={param.shape}")

    # --- 4.4 åŠ è½½æ•°æ®é›† ---
    print("ğŸ“Š æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    train_dataset = BUSIDataset(
        json_path=data_args.train_json,
        image_root=data_args.image_root,
        processor=processor,
        tokenizer=tokenizer,
        max_length=data_args.max_length,
    )

    data_collator = DataCollatorForHulumed(tokenizer=tokenizer)

    # --- 4.5 åˆå§‹åŒ– Trainer ---
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    # --- 4.6 å¼€å§‹è®­ç»ƒ ---
    print("\n" + "=" * 60)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ Hulu-Med LoRA æ¨¡å‹...")
    print("=" * 60 + "\n")

    trainer.train()

    # --- 4.7 ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    final_save_path = os.path.join(training_args.output_dir, "final_model")
    if model_args.probe_position is not None:
        # 1. åªä¿å­˜æ¢é’ˆæƒé‡
        print(f"ğŸ’¾ æ­£åœ¨å•ç‹¬ä¿å­˜æ¢é’ˆæƒé‡...")
        meta_model = model.model if hasattr(model, 'model') else model
        if hasattr(meta_model, 'model'):
            meta_model = meta_model.model

        if hasattr(meta_model, 'probe'):
            probe_weights = meta_model.probe.state_dict()
            # æ ¹æ®ä½ç½®å’Œç±»å‹åŠ¨æ€å‘½åæ–‡ä»¶å
            probe_filename = f"probe_model_{model_args.probe_position}_{model_args.probe_type}.bin"
            save_path = os.path.join(training_args.output_dir, probe_filename)
            torch.save(probe_weights, save_path)
            print(f"âœ… æ¢é’ˆæƒé‡å·²ä¿å­˜è‡³: {save_path}")

        # 2. å¦‚æœä½ åªæƒ³ä¿å­˜æ¢é’ˆï¼Œç”šè‡³å¯ä»¥è·³è¿‡ä¸‹é¢çš„ trainer.save_model(final_save_path)
        # ä½†å»ºè®®ä¿ç•™ï¼Œå› ä¸ºå®ƒä¼šä¿å­˜è®­ç»ƒé…ç½®å’Œæ—¥å¿—
        # trainer.save_model(final_save_path)
    else:
        # æ­£å¸¸ä¿å­˜ LoRA æ¨¡å‹
        trainer.save_model(final_save_path)
    tokenizer.save_pretrained(final_save_path)

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {final_save_path}")


if __name__ == "__main__":
    parser = HfArgumentParser((ModelArguments, DataArguments))
    model_args, data_args = parser.parse_args_into_dataclasses()
    train(model_args, data_args)
